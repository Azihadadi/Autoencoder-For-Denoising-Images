{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoders for Denoising Images**\n",
        "\n",
        "Building autoencoders using Keras for denoising data"
      ],
      "metadata": {
        "id": "uHar6iYrjyj6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DviGtlmAjxsX"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.16.2\n",
        "!pip install matplotlib==3.9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the MNIST dataset for training by normalizing the pixel values and flattening the images."
      ],
      "metadata": {
        "id": "o4o7pNLFkNRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "metadata": {
        "id": "HrIimZLikAc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Random Noise to the Dataset**"
      ],
      "metadata": {
        "id": "EJEUGQMTkrBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add noise to the data\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ],
      "metadata": {
        "id": "BzX_WNRYkl3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Autoencoder Model**\n",
        "\n",
        "This model includes an encoder that compresses the input to 64 dimensions and a decoder that reconstructs the input from these 64 dimensions."
      ],
      "metadata": {
        "id": "LMbKAtoGkVr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Encoder\n",
        "input_layer = Input(shape=(784,))\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "\n",
        "# Bottleneck\n",
        "bottleneck = Dense(64, activation='relu')(encoded)\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(64, activation='relu')(bottleneck)\n",
        "output_layer = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Summary of the model\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "DhGKR4rfk1JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Denoising Images with Autoencoder**\n",
        "\n",
        "Training the autoencoder to denoise the images. The autoencoder learns to reconstruct the original images from the noisy input, which can be visualized by comparing the noisy, denoised, and original images."
      ],
      "metadata": {
        "id": "TZMKjfjtlXZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder with noisy data\n",
        "autoencoder.fit(\n",
        "    x_train_noisy, x_train,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test_noisy, x_test)\n",
        ")\n",
        "\n",
        "# Denoise the test images\n",
        "reconstructed_noisy = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Visualize the results\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    # Display noisy images\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display denoised images\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(reconstructed_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display original images\n",
        "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LDZO7QkNlRMg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}